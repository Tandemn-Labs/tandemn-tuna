[build-system]
requires = ["setuptools>=68.0"]
build-backend = "setuptools.build_meta"

[project]
name = "tandemn-tuna"
version = "0.0.1a3"
description = "Hybrid GPU Inference Orchestrator â€” serverless for cold starts, spot for scale"
readme = "README.md"
license = "MIT"
requires-python = ">=3.11"
keywords = ["gpu", "serverless", "spot", "inference", "vllm", "openai", "router"]
authors = [
    {name = "Hetarth", email = "hetarth@tandemn.com"},
    {name = "Mankeerat", email = "mankeerat@tandemn.com"},
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
dependencies = [
    "flask>=3.0",
    "requests>=2.31",
    "gunicorn>=21.0",
    "pyyaml>=6.0",
    "skypilot[aws]>=0.7",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0",
    "pytest-mock>=3.12",
]
cloudrun = ["google-cloud-run>=0.10.0"]
modal = ["modal>=0.73"]
all = [
    "google-cloud-run>=0.10.0",
    "modal>=0.73",
]

[project.scripts]
tuna = "tuna.__main__:main"

[project.urls]
Homepage = "https://github.com/Tandemn-Labs/tandemn-tuna"
Repository = "https://github.com/Tandemn-Labs/tandemn-tuna"
Issues = "https://github.com/Tandemn-Labs/tandemn-tuna/issues"

[tool.setuptools.packages.find]
include = ["tuna*"]

[tool.pytest.ini_options]
testpaths = ["tests"]
