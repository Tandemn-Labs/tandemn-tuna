[build-system]
requires = ["setuptools>=68.0"]
build-backend = "setuptools.build_meta"

[project]
name = "tandemn-tuna"
version = "0.0.1a8"
description = "Hybrid GPU Inference Orchestrator â€” serverless for cold starts, spot for scale"
readme = "README.md"
license = "MIT"
requires-python = ">=3.11"
keywords = ["gpu", "serverless", "spot", "inference", "vllm", "openai", "router"]
authors = [
    {name = "Hetarth", email = "hetarth@tandemn.com"},
    {name = "Mankeerat", email = "mankeerat@tandemn.com"},
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
dependencies = [
    "flask>=3.0",
    "requests>=2.31",
    "gunicorn>=21.0",
    "pyyaml>=6.0",
    "rich>=13.0",
    "skypilot[aws]>=0.11,<0.12",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0",
    "pytest-mock>=3.12",
]
cloudrun = ["google-cloud-run>=0.10.0"]
modal = ["modal>=0.73"]
baseten = ["truss>=0.9"]
azure = ["azure-mgmt-appcontainers>=4.0", "azure-identity>=1.15"]
cerebrium = ["cerebrium"]
gcp-spot = ["skypilot[gcp]>=0.11,<0.12"]
all = [
    "google-cloud-run>=0.10.0",
    "modal>=0.73",
    "truss>=0.9",
    "azure-mgmt-appcontainers>=4.0",
    "azure-identity>=1.15",
    "cerebrium",
    "skypilot[gcp]>=0.11,<0.12",
]

[project.scripts]
tuna = "tuna.__main__:main"

[project.urls]
Homepage = "https://github.com/Tandemn-Labs/tandemn-tuna"
Repository = "https://github.com/Tandemn-Labs/tandemn-tuna"
Issues = "https://github.com/Tandemn-Labs/tandemn-tuna/issues"

[tool.setuptools.packages.find]
include = ["tuna*"]

[tool.setuptools]
include-package-data = true

[tool.pytest.ini_options]
testpaths = ["tests"]
